{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "# from scipy.misc import imresize\n",
    "from imageio import imread, imsave\n",
    "from natsort import natsorted\n",
    "\n",
    "# from labelConv import label2int, int2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour_precedence(contour, cols):\n",
    "    tolerance_factor = 10\n",
    "    origin = cv2.boundingRect(contour)\n",
    "    return ((origin[1] // tolerance_factor) * tolerance_factor) * cols + origin[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour_precedence_two(contour, cols):\n",
    "    tolerance_factor = 10\n",
    "    origin = cv2.boundingRect(contour)\n",
    "    return ((origin[0] // tolerance_factor) * tolerance_factor) * cols + origin[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captch_ex(file_name):\n",
    "    img = cv2.imread(file_name)\n",
    "    img = cv2.resize(img,None,fx=0.3, fy=0.3, interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "    img_final = cv2.imread(file_name)\n",
    "    img2gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, mask = cv2.threshold(img2gray, 180, 255, cv2.THRESH_BINARY)\n",
    "    image_final = cv2.bitwise_and(img2gray, img2gray, mask=mask)\n",
    "    ret, new_img = cv2.threshold(image_final, 180, 255, cv2.THRESH_BINARY)  # for black text , cv.THRESH_BINARY_INV\n",
    "    '''\n",
    "            line  8 to 12  : Remove noisy portion \n",
    "    '''\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3,\n",
    "                                                         3))  # to manipulate the orientation of dilution , large x means horizonatally dilating  more, large y means vertically dilating more\n",
    "    dilated = cv2.dilate(new_img, kernel, iterations=5)  # dilate , more the iteration more the dilation\n",
    "\n",
    "    # for cv2.x.x\n",
    "\n",
    "    _, contours,hierarchy = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)  # get contours\n",
    "    \n",
    "    #print(contours[0].shape)\n",
    "    contours.sort(key=lambda x:get_contour_precedence(x, img.shape[1]))\n",
    "    \n",
    "    # for cv3.x.x comment above line and uncomment line below\n",
    "\n",
    "    #image, contours, hierarchy = cv2.findContours(dilated,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    index = 1\n",
    "    for contour in contours:\n",
    "        # get rectangle bounding contour\n",
    "        [x, y, w, h] = cv2.boundingRect(contour)\n",
    "\n",
    "        if(w*h < 800):\n",
    "            continue\n",
    "        # draw rectangle around contour on original image\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 255), 1)\n",
    "\n",
    "        \n",
    "        #you can crop image and send to OCR  , false detected will return no text :)\n",
    "        cropped = img[y :y +  h , x : x + w]\n",
    "        #print(file_name.split('.')[0])\n",
    "        s = 'data/crop_' + str(index) + '.jpg' \n",
    "        #print(s)\n",
    "        cv2.imwrite(s , cropped)\n",
    "        #cv2.imshow('captcha_result', cropped)\n",
    "        #cv2.waitKey()\n",
    "        index = index + 1\n",
    "\n",
    "        \n",
    "    #print(index-1)\n",
    "    # write original image with added contours to disk\n",
    "    #cv2.imshow('captcha_result', img)\n",
    "    #cv2.waitKey()\n",
    "    return index-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captch_ex2(file_name, index):\n",
    "    img = cv2.imread(file_name)\n",
    "    img = cv2.resize(img,None,fx=1, fy=1, interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "    img_final = cv2.imread(file_name)\n",
    "    img2gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, mask = cv2.threshold(img2gray, 180, 255, cv2.THRESH_BINARY)\n",
    "    image_final = cv2.bitwise_and(img2gray, img2gray, mask=mask)\n",
    "    ret, new_img = cv2.threshold(image_final, 180, 255, cv2.THRESH_BINARY)  # for black text , cv.THRESH_BINARY_INV\n",
    "    '''\n",
    "            line  8 to 12  : Remove noisy portion \n",
    "    '''\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (1,\n",
    "                                                         1))  # to manipulate the orientation of dilution , large x means horizonatally dilating  more, large y means vertically dilating more\n",
    "    dilated = cv2.dilate(new_img, kernel, iterations=0)  # dilate , more the iteration more the dilation\n",
    "\n",
    "    # for cv2.x.x\n",
    "\n",
    "    _, contours,hierarchy = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)  # get contours\n",
    "\n",
    "    # for cv3.x.x comment above line and uncomment line below\n",
    "\n",
    "    #image, contours, hierarchy = cv2.findContours(dilated,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "    contours.sort(key=lambda x:get_contour_precedence_two(x, img.shape[1]))\n",
    "    index2 = 1\n",
    "    for contour in contours:\n",
    "        # get rectangle bounding contour\n",
    "        [x, y, w, h] = cv2.boundingRect(contour)\n",
    "\n",
    "        # draw rectangle around contour on original image\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 255), 1)\n",
    "\n",
    "        \n",
    "        #you can crop image and send to OCR  , false detected will return no text :)\n",
    "        cropped = img[y :y +  h , x : x + w]\n",
    "        #print(file_name.split('.')[0])\n",
    "        s = 'data/test/crop_' + str(index) + '_' + str(index2) + '.jpg' \n",
    "        #print(s)\n",
    "        cv2.imwrite(s , cropped)\n",
    "        #cv2.imshow('captcha_result', cropped)\n",
    "        #cv2.waitKey()\n",
    "        index2 = index2 + 1\n",
    "\n",
    "        \n",
    "    #print(index2-1)\n",
    "    # write original image with added contours to disk\n",
    "    #cv2.imshow('captcha_result', img)\n",
    "    #cv2.waitKey()\n",
    "    return index2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearFolder(dirPath):\n",
    "    fileList = os.listdir(dirPath)\n",
    "    for fileName in fileList:\n",
    "        os.remove(dirPath+\"/\"+fileName)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/shyam/MyCode/text_recognition/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Input image dimensions\n",
    "img_rows, img_cols = 20, 20\n",
    "batch_size = 128\n",
    "nb_classes = 62  # A-Z, a-z and 0-9\n",
    "nb_epoch = 500\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal', activation='relu', input_shape=(img_rows, img_cols, 1)))\n",
    "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Conv2D(512, (3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Conv2D(512, (3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes, kernel_initializer='he_normal', activation='softmax'))\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'best.kerasModelWeights', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5f60dd87e00b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best.kerasModelWeights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/MyCode/text_recognition/venv/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyCode/text_recognition/venv/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyCode/text_recognition/venv/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyCode/text_recognition/venv/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'best.kerasModelWeights', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"best.kerasModelWeights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'sample.jpg'\n",
    "num_images = captch_ex(file_name)\n",
    "#num = captch_ex2('data/crop_8.jpg', 8)\n",
    "clearFolder('data/test')\n",
    "records = []\n",
    "\n",
    "for i in range(num_images):\n",
    "    path = 'data/crop_'\n",
    "    new_filename = path + str(i+1) + '.jpg'\n",
    "    num_char = captch_ex2(new_filename, index=i+1)\n",
    "    records.append((i+1, num_char))\n",
    "    \n",
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(FILEPATH):\n",
    "\n",
    "    \n",
    "    #path = \"data\"\n",
    "    path = FILEPATH + '/test'\n",
    "    # Input image dimensions\n",
    "    img_rows, img_cols = 20, 20\n",
    "\n",
    "    # Keep or not the initial image aspect ratio\n",
    "    keepRatio = False\n",
    "\n",
    "    # Suffix of the created directories and files\n",
    "    suffix = \"Preproc_\" + str(img_rows) + \"_\" + str(img_cols) + (\"_kr\" if keepRatio else \"\")\n",
    "\n",
    "\n",
    "    # We have to make sure files are sorted according to labels, even if they don't have trailing zeros\n",
    "    files = natsorted(glob.glob(path + \"/*\"))\n",
    "\n",
    "    data = np.zeros((len(files), img_rows, img_cols))  # will add the channel dimension later\n",
    "\n",
    "    for i, filepath in enumerate(files):\n",
    "        #print(filepath)\n",
    "        ext = filepath.split('.')[1]\n",
    "        if(ext == \"db\"):\n",
    "            i-=1\n",
    "            break\n",
    "\n",
    "        image = imread(filepath, True)  # True: flatten to grayscale\n",
    "        if keepRatio:\n",
    "            # Find the largest dimension (height or width)\n",
    "            maxSize = max(image.shape[0], image.shape[1])\n",
    "\n",
    "            # Size of the resized image, keeping aspect ratio\n",
    "            imageWidth = math.floor(img_rows * image.shape[0] / maxSize)\n",
    "            imageHeigh = math.floor(img_cols * image.shape[1] / maxSize)\n",
    "\n",
    "            # Compute deltas to center image (should be 0 for the largest dimension)\n",
    "            dRows = (img_rows - imageWidth) // 2\n",
    "            dCols = (img_cols - imageHeigh) // 2\n",
    "\n",
    "            imageResized = np.zeros((img_rows, img_cols))\n",
    "            imageResized[dRows:dRows + imageWidth, dCols:dCols + imageHeigh] = imresize(image, (imageWidth, imageHeigh))\n",
    "\n",
    "            # Fill the empty image with the median value of the border pixels\n",
    "            # This value should be close to the background color\n",
    "            val = np.median(np.append(imageResized[dRows, :],\n",
    "                                      (imageResized[dRows + imageWidth - 1, :],\n",
    "                                       imageResized[:, dCols],\n",
    "                                       imageResized[:, dCols + imageHeigh - 1])))\n",
    "\n",
    "            # If rows were left blank\n",
    "            if (dRows > 0):\n",
    "                imageResized[0:dRows, :].fill(val)\n",
    "                imageResized[dRows + imageWidth:, :].fill(val)\n",
    "\n",
    "            # If columns were left blank\n",
    "            if (dCols > 0):\n",
    "                imageResized[:, 0:dCols].fill(val)\n",
    "                imageResized[:, dCols + imageHeigh:].fill(val)\n",
    "        else:\n",
    "            imageResized = imresize(image, (img_rows, img_cols))\n",
    "\n",
    "        # Add the resized image to the dataset\n",
    "        data[i] = imageResized\n",
    "\n",
    "        # Save image (mostly for visualization)\n",
    "        filename = filepath.split(\"\\\\\")[-1]\n",
    "        filenameDotSplit = filename.split(\".\")\n",
    "        newFilename = filenameDotSplit[0].zfill(5) + \".\" + filenameDotSplit[-1].lower()  # Add trailing zeros\n",
    "        newName = \"/\".join(filepath.split(\"\\\\\")[:-1]) + \"/\" + newFilename\n",
    "        imsave(newName, imageResized)\n",
    "\n",
    "    # Add channel/filter dimension\n",
    "    data = data[:, :, :, np.newaxis]\n",
    "\n",
    "    # Makes values floats between 0 and 1 (gives better results for neural nets)\n",
    "    data = data.astype('float32')\n",
    "    data /= 255\n",
    "    print(data.shape)\n",
    "    # Save the data as numpy file for faster loading\n",
    "    np.save(FILEPATH + \"/\" + suffix + \".npy\", data)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_npy_file = 'data/Preproc_20_20.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(model, path_to_npy_file, records):\n",
    "    X = np.load(path_to_npy_file)\n",
    "    Y = model.predict_classes(X)\n",
    "    \n",
    "    # Translate integers to character labels\n",
    "    vInt2label = np.vectorize(int2label)\n",
    "    Y = vInt2label(Y)\n",
    "    \n",
    "    list_of_words = []\n",
    "    k = 0\n",
    "    for i in range(len(records)):\n",
    "        s = ''\n",
    "        for j in range(records[i][1]):\n",
    "            c = Y[k].lower()\n",
    "            if(c == 'j'):\n",
    "                s += ''\n",
    "            else:\n",
    "                s += c\n",
    "            k += 1\n",
    "        if(i in [5,6,7,9,10,11,16,17,18,19,20,22,23]):\n",
    "            list_of_words.append(s.lower())\n",
    "    \n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = CNN(model, path_to_npy_file, records)\n",
    "len(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
